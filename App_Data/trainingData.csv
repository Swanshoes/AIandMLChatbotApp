Question,Answer
"What is the first step in building an ML model?","The first step is problem definition, where you clearly identify the business problem, determine if ML is appropriate, and define success metrics."
"How do you split data for training, validation, and testing?","Common splits are 70-15-15 or 80-10-10 for training, validation, and test sets respectively, though this varies based on dataset size."
"What is overfitting in machine learning?","Overfitting occurs when a model learns the training data too well, including noise and outliers, resulting in poor performance on new, unseen data."
"How can you prevent overfitting?","Prevention methods include regularization, dropout, early stopping, data augmentation, cross-validation, and reducing model complexity."
"What is the difference between supervised and unsupervised learning?","Supervised learning uses labeled data to learn input-output mappings, while unsupervised learning finds patterns in unlabeled data without predefined outputs."
"What is a neural network?","A neural network is a computational model inspired by biological neurons, consisting of interconnected layers of nodes that process information through weighted connections."
"What is gradient descent?","Gradient descent is an optimization algorithm that iteratively adjusts model parameters by moving in the direction of steepest descent of the loss function."
"What is the learning rate in ML?","The learning rate is a hyperparameter that controls the step size during gradient descent, determining how quickly or slowly a model learns."
"What is batch normalization?","Batch normalization is a technique that normalizes layer inputs across mini-batches, stabilizing learning, allowing higher learning rates, and reducing training time."
"What is transfer learning?","Transfer learning involves using a pre-trained model on a related task as a starting point, then fine-tuning it for your specific problem to save time and resources."
"What is data augmentation?","Data augmentation artificially increases training data diversity by applying transformations like rotation, flipping, cropping, or noise addition to existing samples."
"What is cross-validation?","Cross-validation is a technique that splits data into k folds, training on k-1 folds and validating on the remaining fold, rotating through all combinations."
"What is feature engineering?","Feature engineering is the process of creating, transforming, or selecting input variables to improve model performance and capture relevant patterns in data."
"What is dimensionality reduction?","Dimensionality reduction decreases the number of input features while preserving important information, using techniques like PCA, t-SNE, or autoencoders."
"What is regularization in ML?","Regularization adds penalty terms to the loss function to discourage model complexity, helping prevent overfitting through techniques like L1, L2, or elastic net."
"What is dropout in neural networks?","Dropout randomly deactivates neurons during training with a specified probability, forcing the network to learn robust features and preventing co-adaptation."
"What is a confusion matrix?","A confusion matrix is a table showing true positives, true negatives, false positives, and false negatives, used to evaluate classification model performance."
"What is precision in ML?","Precision measures the proportion of predicted positive cases that are actually positive, calculated as true positives divided by all predicted positives."
"What is recall in ML?","Recall measures the proportion of actual positive cases correctly identified, calculated as true positives divided by all actual positives."
"What is the F1 score?","The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both measures, especially useful for imbalanced datasets."
"What is a hyperparameter?","A hyperparameter is a configuration setting set before training begins, such as learning rate, batch size, or number of layers, not learned from data."
"What is hyperparameter tuning?","Hyperparameter tuning is the process of finding optimal hyperparameter values through methods like grid search, random search, or Bayesian optimization."
"What is early stopping?","Early stopping halts training when validation performance stops improving for a specified number of epochs, preventing overfitting and saving computational resources."
"What is a loss function?","A loss function quantifies the difference between predicted and actual values, guiding the optimization process by providing a measure to minimize."
"What is backpropagation?","Backpropagation is an algorithm that calculates gradients of the loss function with respect to weights by applying the chain rule backward through the network."
"What is an activation function?","An activation function introduces non-linearity into neural networks, determining whether and how strongly a neuron fires based on its input."
"What are common activation functions?","Common activation functions include ReLU, sigmoid, tanh, Leaky ReLU, and softmax, each with different properties and use cases."
"What is a convolutional neural network?","A CNN is a deep learning architecture designed for processing grid-like data such as images, using convolutional layers to detect spatial patterns and features."
"What is a recurrent neural network?","An RNN processes sequential data by maintaining hidden states that capture information from previous time steps, useful for time series and natural language."
"What is an LSTM network?","Long Short-Term Memory networks are RNN variants with gating mechanisms that better capture long-term dependencies and mitigate vanishing gradient problems."
"What is attention mechanism?","Attention allows models to focus on relevant parts of input when generating output, weighing different input elements dynamically rather than treating all equally."
"What is a transformer architecture?","Transformers use self-attention mechanisms to process sequences in parallel, forming the basis of modern NLP models like BERT, GPT, and others."
"What is fine-tuning a model?","Fine-tuning involves taking a pre-trained model and continuing training on domain-specific data, adjusting weights to specialize for a particular task."
"What is gradient clipping?","Gradient clipping limits gradient magnitudes during training to prevent exploding gradients, stabilizing training by capping values at a threshold."
"What is weight initialization?","Weight initialization sets initial parameter values before training, using strategies like Xavier, He, or random initialization to facilitate effective learning."
"What is data normalization?","Data normalization scales features to a standard range, typically 0-1 or -1 to 1, improving convergence speed and model stability during training."
"What is standardization in ML?","Standardization transforms features to have zero mean and unit variance, making them comparable and improving optimization for many algorithms."
"What is a validation set used for?","A validation set evaluates model performance during training, guides hyperparameter tuning, and helps detect overfitting without touching the test set."
"What is a test set used for?","A test set provides final, unbiased performance evaluation after all training and tuning is complete, simulating real-world deployment scenarios."
"What is class imbalance?","Class imbalance occurs when some classes have significantly fewer samples than others, potentially biasing models toward majority classes."
"How do you handle class imbalance?","Techniques include oversampling minorities, undersampling majorities, synthetic data generation like SMOTE, class weights, or using appropriate evaluation metrics."
"What is ensemble learning?","Ensemble learning combines multiple models to improve predictions, using techniques like bagging, boosting, or stacking to leverage diverse model strengths."
"What is bagging in ML?","Bagging trains multiple models on bootstrap samples of data and averages predictions, reducing variance and improving stability, as in Random Forests."
"What is boosting in ML?","Boosting sequentially trains weak learners, each focusing on examples previous models misclassified, creating a strong ensemble through weighted combination."
"What is a Random Forest?","Random Forest is an ensemble of decision trees trained on random data subsets with random feature selection, combining predictions for robust results."
"What is XGBoost?","XGBoost is an efficient gradient boosting implementation using regularization, parallel processing, and tree pruning to achieve high performance on structured data."
"What is feature scaling?","Feature scaling adjusts the range of features to similar scales, preventing features with larger ranges from dominating distance-based or gradient-based algorithms."
"What is one-hot encoding?","One-hot encoding converts categorical variables into binary vectors, where each category becomes a column with 1 indicating presence and 0 absence."
"What is embedding in ML?","Embeddings map discrete items like words or categories into continuous vector spaces, capturing semantic relationships in lower-dimensional representations."
"What is curriculum learning?","Curriculum learning trains models by presenting examples in meaningful order, starting with easier examples and gradually increasing difficulty for better learning."
"What is data leakage?","Data leakage occurs when training data contains information about the target that wouldn't be available during prediction, causing inflated performance estimates."
"What is k-fold cross-validation?","K-fold CV divides data into k equal parts, trains k models using each fold once as validation while others train, then averages results."
"What is stratified sampling?","Stratified sampling maintains class proportions when splitting data, ensuring each subset has representative distributions of all target classes."
"What is the vanishing gradient problem?","Vanishing gradients occur when gradients become extremely small in deep networks, preventing effective weight updates in earlier layers during backpropagation."
"What is the exploding gradient problem?","Exploding gradients occur when gradients become extremely large during training, causing unstable updates and numerical overflow in neural networks."
"What is batch size in training?","Batch size determines how many samples are processed before updating weights, affecting training speed, memory usage, and convergence behavior."
"What is an epoch in ML training?","An epoch is one complete pass through the entire training dataset, with models typically trained for multiple epochs until convergence."
"What is stochastic gradient descent?","SGD updates weights after each training example rather than the full dataset, introducing noise but enabling faster updates and better generalization."
"What is mini-batch gradient descent?","Mini-batch gradient descent updates weights using small batches of examples, balancing computational efficiency with stable gradient estimates."
"What is Adam optimizer?","Adam combines momentum and adaptive learning rates, maintaining per-parameter learning rates that adapt based on first and second moment estimates of gradients."
"What is momentum in optimization?","Momentum accelerates gradient descent by accumulating a velocity vector in directions of persistent gradient reduction, helping overcome local minima."
"What is learning rate scheduling?","Learning rate scheduling adjusts the learning rate during training, typically decreasing it over time to enable fine-tuning and better convergence."
"What is a residual connection?","Residual connections add input directly to layer output, allowing gradients to flow through skip connections and enabling training of very deep networks."
"What is batch vs online learning?","Batch learning trains on entire datasets at once, while online learning updates models incrementally as new data arrives, adapting continuously."
"What is active learning?","Active learning selectively queries informative examples for labeling, maximizing model improvement while minimizing labeling effort through strategic sample selection."
"What is semi-supervised learning?","Semi-supervised learning uses both labeled and unlabeled data, leveraging large amounts of cheap unlabeled data alongside limited labeled examples."
"What is self-supervised learning?","Self-supervised learning creates supervision signals from unlabeled data itself, such as predicting masked words or image rotations, for pre-training."
"What is reinforcement learning?","Reinforcement learning trains agents through interaction with environments, learning policies that maximize cumulative rewards through trial and error."
"What is a policy in RL?","A policy defines an agent's behavior, mapping states to actions, and can be deterministic or stochastic depending on the RL approach."
"What is reward shaping in RL?","Reward shaping modifies reward functions to guide learning more effectively, providing intermediate rewards to encourage desired behaviors and faster learning."
"What is exploration vs exploitation?","Exploration tries new actions to discover better strategies, while exploitation uses known good actions, balancing both is crucial in RL."
"What is Q-learning?","Q-learning is a model-free RL algorithm that learns action-value functions, estimating expected returns for state-action pairs to derive optimal policies."
"What is policy gradient?","Policy gradient methods directly optimize policy parameters by computing gradients of expected rewards, enabling learning of stochastic policies and continuous actions."
"What is data preprocessing?","Data preprocessing cleans, transforms, and prepares raw data for training, including handling missing values, outliers, scaling, and encoding."
"What is missing data imputation?","Imputation fills missing values using strategies like mean, median, mode, forward fill, interpolation, or predictive models based on other features."
"What is feature selection?","Feature selection identifies and retains the most relevant features, reducing dimensionality, improving interpretability, and potentially enhancing model performance."
"What is model compression?","Model compression reduces model size and computational requirements through techniques like pruning, quantization, knowledge distillation, or low-rank factorization."
"What is knowledge distillation?","Knowledge distillation trains smaller student models to mimic larger teacher models, transferring learned knowledge while maintaining efficiency."
"What is quantization in ML?","Quantization reduces numerical precision of weights and activations from floating-point to lower-bit representations, decreasing memory and computation requirements."
"What is pruning in neural networks?","Pruning removes unnecessary weights or neurons based on importance criteria, creating sparser networks that maintain accuracy with reduced parameters."
"What is a generative model?","Generative models learn to produce new data samples similar to training data, modeling underlying data distributions for synthesis tasks."
"What is a discriminative model?","Discriminative models learn decision boundaries between classes, directly modeling conditional probability of outputs given inputs for classification tasks."
"What is a GAN?","Generative Adversarial Networks consist of generator and discriminator networks competing, with the generator learning to create realistic samples."
"What is a VAE?","Variational Autoencoders encode data into latent distributions and decode back, learning compressed representations that can generate new samples."
"What is contrastive learning?","Contrastive learning trains models by pulling similar examples together and pushing dissimilar ones apart in representation space."
"What is data drift?","Data drift occurs when statistical properties of production data differ from training data, potentially degrading model performance over time."
"What is model monitoring?","Model monitoring tracks deployed model performance, data quality, and system health, detecting issues like drift, bias, or degradation."
"What is A/B testing for models?","A/B testing compares model versions by randomly assigning users to different models and measuring business metrics to determine which performs better."
"What is model versioning?","Model versioning tracks different iterations of models, including code, hyperparameters, and training data, enabling reproducibility and rollback capabilities."
"What is MLOps?","MLOps applies DevOps principles to ML, establishing practices for deployment, monitoring, versioning, and automation of ML systems in production."
"What is feature store?","A feature store centralizes feature definitions and serves consistent features for training and inference, ensuring feature reusability and consistency."
"What is model interpretability?","Model interpretability explains how models make decisions, using techniques like SHAP, LIME, or attention visualization to understand predictions."
"What is SHAP values?","SHAP values quantify each feature's contribution to individual predictions using game theory, providing unified interpretability across model types."
"What is model bias?","Model bias occurs when models systematically favor certain groups or outcomes, potentially perpetuating or amplifying societal biases present in training data."
"What is fairness in ML?","Fairness ensures models treat different groups equitably, measured through metrics like demographic parity, equal opportunity, or equalized odds."
"What is federated learning?","Federated learning trains models across decentralized devices without centralizing data, preserving privacy by sharing only model updates."
"What is differential privacy?","Differential privacy adds controlled noise to computations, providing mathematical guarantees that individual data points cannot be identified from model outputs."
"What is neural architecture search?","NAS automatically discovers optimal network architectures using methods like reinforcement learning, evolution, or gradient-based optimization."
"What is meta-learning?","Meta-learning trains models to learn how to learn, enabling quick adaptation to new tasks with minimal examples by learning across task distributions."
"What is few-shot learning?","Few-shot learning trains models to recognize new classes from very few examples, typically using metric learning or meta-learning approaches."